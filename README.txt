clustering_xxx files implement kmeans and Gaussian Mixture (expectation maximization) on a dataset and plot an elbow curve of sum of square distances,
summed over each x and the centroid of that x's assigned cluster as a measure of cluster compactness.

dim_reduc_xxx files implement PCA, ICA, Random Projections, and Recursive Feature Elimination on a dataset and right now collect and print some things to get a sense of sklearn's attributes for a given class,
often varying the number of components to be selected (from 1 up to the original number of features).

Eventually various combinations of clustering and dimensional reduction/feature transformation will be used to transform the datasets before applying a NN.

data.py has methods that gather and preprocess the data for the tennis (win/loss) and titanic (survival) datasets. For the tennis data in particular, many columns are dropped for various reasons.
The original tennis data has winning player stats listed first, so those rows are duplicated with player-specific stats flipped and outcome changed to a loss, then shuffled to give an equal number of
positive and negative labels.
Non-numerical categorical features without a clear order are one-hot encoded. The data is then normalized. The earlier files use them to load the datasets.

support.py has some ancillary methods.

pca_titanic.png is generated by dim_reduc_titanic and just plots the singular values from PCA on the titanic dataset in order.